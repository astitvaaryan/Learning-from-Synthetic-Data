# Learning-from-Synthetic-Data
This project explores the potential of using synthetic data generated by advanced generative models to achieve various downstream machine learning tasks. The key idea is to investigate whether synthetic data can be effectively used for applications such as classification, segmentation, etc.

## Overview
With the advancement of generative models like GANs, we can generate an unbounded supply of synthetic data. This synthetic data offers several advantages over traditional fixed real datasets, including enhanced diversity, improved quality, and reduced storage requirements (e.g., BigGAN takes 225MB, while ImageNet requires 150GB). This project aims to harness these benefits to improve model performance and robustness across different applications.

### Methodology
1. **Hard Sample Mining (HSM) with latent code optimization, Dataset Smoothing, and Batch Norm statistics Adaptation (BNA)**
<div align="center">
   <img src="https://github.com/astitvaaryan/Learning-from-Synthetic-Data/blob/main/Training%20Models%20from%20Generated%20Images.png" alt="Framework1" width="750"/>
</div>

This method explores the effectiveness of training machine learning models using synthetic data generated by advanced methods such as GANs. It highlights techniques to create high-quality synthetic datasets and demonstrates their utility in various training scenarios. The goal is to train a classifier solely on the generated data and minimize the performance gap when compared to training on real data.

**Key Techniques Implemented:**
- **Hard Sample Mining (HSM) with Latent Code Optimization:** Hard Sample Mining focuses on using the most challenging images (those which are difficult to classify correctly) to train the classifier. This is achieved by minimizing the score of the highest confidence class, regardless of the true label. The latent code (z) is optimized while maintaining the same L2 norm to prevent deviation from the Gaussian prior.
- **Dataset Smoothing (DS):** Dataset Smoothing aims to enhance visual diversity by partially replacing generated training data with new samples each epoch. This approach ensures a diverse yet gradually changing dataset, thus improving classifier performance.
- **Batch Norm Statistics Adaptation (BNA):** Batch Normalization stabilizes the distribution of hidden activations by transforming the output of a layer to have zero mean and unit variance. This technique adapts BN statistics to improve training stability and performance when using synthetic data.

We have implemented the following techniques from this paper on the Imagenet-10 dataset:
[This Dataset Does Not Exist: Training Models from Generated Images](https://arxiv.org/pdf/1911.02888)

2. **GAN Distillation**
<div align="center">
    <img src="https://github.com/astitvaaryan/Learning-from-Synthetic-Data/blob/main/Self-improving%20Classification%20Performance%20through%20GAN%20Distillation.png" alt="Framework2" width="250"/>
</div>

This method aims to enhance the performance of an already-trained classifier without the need for additional annotated samples. This is achieved through a novel approach involving GAN distillation, where a GAN generates difficult samples for the classifier to learn from.

**GAN Distillation Process:**

- **Initial Classification:** Samples are divided into hard and easy subsets based on the classifier's performance. A sample is considered hard if it is misclassified or if it is classified correctly but with low confidence.
- **GAN Framework:** The GAN consists of a generator and a discriminator. The discriminator's loss is computed using a binary cross-entropy loss function to differentiate between real and synthetic images. The generator's loss, influenced by the discriminator's feedback, focuses on creating realistic and challenging images.
- **Triplet Loss:** The generator is trained using a triplet loss to ensure the features of generated samples are closer to those of hard samples compared to easy samples.
This encourages the GAN to produce samples that are inherently difficult for the classifier.
- **Training and Fine-Tuning:** During training, the classifier and generator are alternately fine-tuned for several epochs. This iterative process helps the classifier improve by training on synthetic hard samples, while the generator adapts to the classifier's evolving feature space.
- **Result:** The approach was tested using the AlexNet classifier.The GAN distillation method significantly improved classification performance, even with larger fractions of the available training data. By augmenting the dataset with synthetic hard samples, the classifier achieved higher accuracy compared to the baseline.

We have implemented the following techniques from this paper on the Cifar-10 dataset:
[Self-improving classification performance through GAN distillation](https://openaccess.thecvf.com/content/ICCV2021W/ILDAV/papers/Pennisi_Self-Improving_Classification_Performance_Through_GAN_Distillation_ICCVW_2021_paper.pdf)
